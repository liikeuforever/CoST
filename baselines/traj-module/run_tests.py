#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è½¨è¿¹å‹ç¼©ç®—æ³•æµ‹è¯•è„šæœ¬
å¯¹å¤šä¸ªæ•°æ®é›†è¿è¡Œæ‰€æœ‰ç®—æ³•ï¼Œå¹¶æ±‡æ€»å¯¹æ¯”ç»“æœ
"""

import subprocess
import os
import re
import time
from pathlib import Path
import sys
import platform

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.absolute()

# æ£€æµ‹æ“ä½œç³»ç»Ÿ
IS_WINDOWS = platform.system() == 'Windows'
IS_MACOS = platform.system() == 'Darwin'
IS_LINUX = platform.system() == 'Linux'

# æ ¹æ®æ“ä½œç³»ç»Ÿé€‰æ‹©å¯æ‰§è¡Œæ–‡ä»¶
def get_executable_path(base_name, exe_name=None):
    """æ ¹æ®æ“ä½œç³»ç»Ÿè¿”å›å¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„"""
    if IS_WINDOWS:
        return f"{base_name}/x64/Debug/{Path(base_name).name}.exe"
    else:
        # macOS/Linux: ä½¿ç”¨æŒ‡å®šçš„å¯æ‰§è¡Œæ–‡ä»¶å
        if exe_name is None:
            exe_name = Path(base_name).name
        return f"build/{exe_name}"

# ç®—æ³•é…ç½®
ALGORITHMS = [
    {
        'name': 'Dead_Reckoning',
        'executable': get_executable_path('Dead_Reckoning_Test', 'Dead_Reckoning_Test'),
        'source_dir': 'Dead_Reckoning_Test/Dead_Reckoning_Test',
        'source_file': 'Dead_Reckoning_Test.cpp',
        'params': lambda dataset, output: [dataset, '1.0', output, '-1']  # epsilon=1m
    },
    {
        'name': 'Douglas_Peucker',
        'executable': get_executable_path('DouglasPeuckerTest', 'Douglas_Peucker'),
        'source_dir': 'DouglasPeuckerTest/DouglasPeuckerTest',
        'source_file': 'DouglasPeuckerTest.cpp',
        'params': lambda dataset, output: [dataset, '1.0', output, '-1']  # epsilon=1m
    },
    {
        'name': 'OPW_TR',
        'executable': get_executable_path('OPW-TR-Test', 'OPW_TR'),
        'source_dir': 'OPW-TR-Test/OPW-TR-Test',
        'source_file': 'OPW-TR-Test.cpp',
        'params': lambda dataset, output: [dataset, '1.0', output, '-1']  # epsilon=1m
    },
    {
        'name': 'SQUISH_E',
        'executable': get_executable_path('SQUISH_E_Test', 'SQUISH_E'),
        'source_dir': 'SQUISH_E_Test/SQUISH_E_Test',
        'source_file': 'SQUISH_E_Test.cpp',
        'params': lambda dataset, output: [dataset, '2.0', '1.0', output, '-1']  # ratio=2, sed=1m
    },
    {
        'name': 'VOLTCom',
        'executable': get_executable_path('VOLTComTest', 'VOLTCom'),
        'source_dir': 'VOLTComTest/VOLTComTest',
        'source_file': 'VOLTComTest.cpp',
        'params': lambda dataset, output: [dataset, '1.0', output, '-1']  # epsilon=1m
    }
]

# æ•°æ®é›†é…ç½®
DATASETS = [
    {
        'name': 'Geolife',
        'file': 'data_set/Geolife_100k_with_id.csv'
    },
    {
        'name': 'Trajectory',
        'file': 'data_set/Trajtory_100k_with_id.csv'
    },
    {
        'name': 'WX_taxi',
        'file': 'data_set/WX_taxi_100k_with_id.csv'
    }
]

def parse_output_file(output_file):
    """
    è§£æç®—æ³•è¾“å‡ºæ–‡ä»¶ï¼Œæå–æ€§èƒ½æŒ‡æ ‡
    """
    results = {
        'original_points': 0,
        'compressed_points': 0,
        'compression_ratio': 0.0,
        'avg_compress_time': 0.0,
        'avg_decompress_time': 0.0,
        'avg_error': 0.0,
        'max_error': 0.0
    }
    
    if not os.path.exists(output_file):
        print(f"  è­¦å‘Š: è¾“å‡ºæ–‡ä»¶ä¸å­˜åœ¨ {output_file}")
        return results
    
    try:
        with open(output_file, 'r', encoding='utf-8') as f:
            content = f.read()
            
            # æå–å„é¡¹æŒ‡æ ‡
            patterns = {
                'original_points': r'Original Points:\s*(\d+)',
                'compressed_points': r'(?:Compressed Points|Compressed Segments|Simplified Points):\s*(\d+)',
                'compression_ratio': r'Compression Ratio:\s*([\d.]+)%',
                'avg_compress_time': r'Average Time per Point:\s*([\d.]+)\s*us/point',
                'avg_decompress_time': r'--- Decompression.*?Average Time per Point:\s*([\d.]+)\s*us/point',
                'avg_error': r'Average Error:\s*([\d.]+)\s*m',
                'max_error': r'Maximum Error:\s*([\d.]+)\s*m'
            }
            
            for key, pattern in patterns.items():
                match = re.search(pattern, content, re.DOTALL)
                if match:
                    results[key] = float(match.group(1))
    
    except Exception as e:
        print(f"  é”™è¯¯: è§£æè¾“å‡ºæ–‡ä»¶å¤±è´¥ {output_file}: {e}")
    
    return results

def compile_algorithm(algorithm):
    """
    åœ¨ macOS/Linux ä¸Šç¼–è¯‘ C++ æºä»£ç 
    """
    if IS_WINDOWS:
        return True  # Windows ä¸Šä½¿ç”¨é¢„ç¼–è¯‘çš„ exe
    
    algo_name = algorithm['name']
    source_dir = PROJECT_ROOT / algorithm['source_dir']
    source_file = source_dir / algorithm['source_file']
    
    if not source_file.exists():
        print(f"  âš ï¸  æºæ–‡ä»¶ä¸å­˜åœ¨: {source_file}")
        return False
    
    # åˆ›å»º build ç›®å½•
    build_dir = PROJECT_ROOT / "build"
    build_dir.mkdir(exist_ok=True)
    
    # è¾“å‡ºå¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„
    output_exe = build_dir / algo_name
    
    # å¦‚æœå·²ç¼–è¯‘ä¸”è¾ƒæ–°ï¼Œè·³è¿‡
    if output_exe.exists() and output_exe.stat().st_mtime > source_file.stat().st_mtime:
        return True
    
    print(f"  ğŸ“¦ æ­£åœ¨ç¼–è¯‘ {algo_name}...")
    
    # ç¼–è¯‘å‘½ä»¤
    compile_cmd = [
        'g++',
        '-std=c++11',
        '-O2',
        str(source_file),
        '-o', str(output_exe)
    ]
    
    try:
        result = subprocess.run(
            compile_cmd,
            capture_output=True,
            text=True,
            timeout=60
        )
        
        if result.returncode != 0:
            print(f"  âŒ ç¼–è¯‘å¤±è´¥:")
            print(f"     {result.stderr[:500]}")
            return False
        
        print(f"  âœ“ ç¼–è¯‘æˆåŠŸ: {output_exe}")
        return True
        
    except Exception as e:
        print(f"  âŒ ç¼–è¯‘å¼‚å¸¸: {e}")
        return False

def run_algorithm(algorithm, dataset, output_dir):
    """
    è¿è¡Œå•ä¸ªç®—æ³•å¯¹å•ä¸ªæ•°æ®é›†çš„æµ‹è¯•
    """
    algo_name = algorithm['name']
    dataset_name = dataset['name']
    
    # åœ¨é Windows å¹³å°ä¸Šå…ˆç¼–è¯‘
    if not IS_WINDOWS:
        if not compile_algorithm(algorithm):
            print(f"  âš ï¸  ç¼–è¯‘å¤±è´¥ï¼Œè·³è¿‡æµ‹è¯•")
            return None
    
    # æ„å»ºå¯æ‰§è¡Œæ–‡ä»¶è·¯å¾„
    exe_path = PROJECT_ROOT / algorithm['executable']
    
    # æ£€æŸ¥å¯æ‰§è¡Œæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not exe_path.exists():
        print(f"  âš ï¸  å¯æ‰§è¡Œæ–‡ä»¶ä¸å­˜åœ¨: {exe_path}")
        return None
    
    # æ„å»ºæ•°æ®é›†è·¯å¾„
    dataset_path = PROJECT_ROOT / dataset['file']
    if not dataset_path.exists():
        print(f"  âš ï¸  æ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")
        return None
    
    # æ„å»ºè¾“å‡ºæ–‡ä»¶è·¯å¾„
    output_file = output_dir / f"{algo_name}_{dataset_name}_output.txt"
    
    # æ„å»ºå‘½ä»¤å‚æ•°
    params = algorithm['params'](str(dataset_path), str(output_file))
    cmd = [str(exe_path)] + params
    
    print(f"  æ­£åœ¨è¿è¡Œ: {algo_name} on {dataset_name}...")
    
    try:
        # è¿è¡Œç®—æ³•
        start_time = time.time()
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            timeout=300,  # 5åˆ†é’Ÿè¶…æ—¶
            encoding='utf-8',
            errors='ignore'
        )
        end_time = time.time()
        
        if result.returncode != 0:
            print(f"  âŒ è¿è¡Œå¤±è´¥ (è¿”å›ç : {result.returncode})")
            print(f"     é”™è¯¯ä¿¡æ¯: {result.stderr[:200]}")
            return None
        
        # è§£æè¾“å‡ºæ–‡ä»¶
        results = parse_output_file(output_file)
        results['wall_time'] = end_time - start_time
        
        print(f"  âœ“ å®Œæˆ (è€—æ—¶: {results['wall_time']:.2f}ç§’)")
        print(f"    å‹ç¼©æ¯”: {results['compression_ratio']:.2f}%")
        print(f"    å¹³å‡å‹ç¼©æ—¶é—´: {results['avg_compress_time']:.4f} us/ç‚¹")
        print(f"    å¹³å‡è§£å‹æ—¶é—´: {results['avg_decompress_time']:.4f} us/ç‚¹")
        
        return results
        
    except subprocess.TimeoutExpired:
        print(f"  â±ï¸  è¶…æ—¶ (>5åˆ†é’Ÿ)")
        return None
    except Exception as e:
        print(f"  âŒ å¼‚å¸¸: {e}")
        return None

def generate_csv_report(all_results, output_dir):
    """
    ç”ŸæˆCSVæ ¼å¼çš„æ±‡æ€»æŠ¥å‘Š
    """
    csv_file = output_dir / "test_results_summary.csv"
    
    with open(csv_file, 'w', encoding='utf-8') as f:
        # å†™å…¥CSVè¡¨å¤´
        f.write("ç®—æ³•,æ•°æ®é›†,åŸå§‹ç‚¹æ•°,å‹ç¼©åç‚¹æ•°,å‹ç¼©æ¯”(%),å‹ç¼©æ—¶é—´(us/ç‚¹),è§£å‹æ—¶é—´(us/ç‚¹),å¹³å‡è¯¯å·®(m),æœ€å¤§è¯¯å·®(m)\n")
        
        # å†™å…¥æ¯ä¸ªæµ‹è¯•çš„ç»“æœ
        for dataset in DATASETS:
            dataset_name = dataset['name']
            for algo in ALGORITHMS:
                algo_name = algo['name']
                key = (algo_name, dataset_name)
                
                if key in all_results:
                    r = all_results[key]
                    f.write(f"{algo_name},{dataset_name},"
                           f"{int(r['original_points'])},{int(r['compressed_points'])},"
                           f"{r['compression_ratio']:.2f},"
                           f"{r['avg_compress_time']:.4f},{r['avg_decompress_time']:.4f},"
                           f"{r['avg_error']:.2f},{r['max_error']:.2f}\n")
                else:
                    f.write(f"{algo_name},{dataset_name},N/A,N/A,N/A,N/A,N/A,N/A,N/A\n")
    
    # ç”Ÿæˆç®—æ³•å¹³å‡æ€§èƒ½CSV
    csv_avg_file = output_dir / "test_results_average.csv"
    
    with open(csv_avg_file, 'w', encoding='utf-8') as f:
        f.write("ç®—æ³•,å¹³å‡å‹ç¼©æ¯”(%),å¹³å‡å‹ç¼©æ—¶é—´(us/ç‚¹),å¹³å‡è§£å‹æ—¶é—´(us/ç‚¹),å¹³å‡è¯¯å·®(m)\n")
        
        for algo in ALGORITHMS:
            algo_name = algo['name']
            algo_results = [all_results[(algo_name, ds['name'])] 
                          for ds in DATASETS 
                          if (algo_name, ds['name']) in all_results]
            
            if algo_results:
                avg_ratio = sum(r['compression_ratio'] for r in algo_results) / len(algo_results)
                avg_comp_time = sum(r['avg_compress_time'] for r in algo_results) / len(algo_results)
                avg_decomp_time = sum(r['avg_decompress_time'] for r in algo_results) / len(algo_results)
                avg_err = sum(r['avg_error'] for r in algo_results) / len(algo_results)
                
                f.write(f"{algo_name},{avg_ratio:.2f},{avg_comp_time:.4f},"
                       f"{avg_decomp_time:.4f},{avg_err:.2f}\n")
            else:
                f.write(f"{algo_name},N/A,N/A,N/A,N/A\n")
    
    print(f"\nâœ“ CSVæŠ¥å‘Šå·²ç”Ÿæˆ:")
    print(f"  - {csv_file}")
    print(f"  - {csv_avg_file}")
    return csv_file, csv_avg_file

def generate_report(all_results, output_dir):
    """
    ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    """
    report_file = output_dir / "test_results_summary.txt"
    
    with open(report_file, 'w', encoding='utf-8') as f:
        f.write("=" * 100 + "\n")
        f.write("è½¨è¿¹å‹ç¼©ç®—æ³•æ€§èƒ½å¯¹æ¯”æµ‹è¯•æŠ¥å‘Š\n")
        f.write("=" * 100 + "\n\n")
        f.write(f"æµ‹è¯•æ—¶é—´: {time.strftime('%Y-%m-%d %H:%M:%S')}\n")
        f.write(f"æµ‹è¯•å‚æ•°: epsilon=1m (Dead_Reckoning, Douglas_Peucker, OPW_TR, VOLTCom)\n")
        f.write(f"          ratio=2, sed=1m (SQUISH_E)\n\n")
        
        # æŒ‰æ•°æ®é›†åˆ†ç»„è¾“å‡º
        for dataset in DATASETS:
            dataset_name = dataset['name']
            f.write("=" * 100 + "\n")
            f.write(f"æ•°æ®é›†: {dataset_name}\n")
            f.write("=" * 100 + "\n\n")
            
            # è¡¨å¤´
            f.write(f"{'ç®—æ³•':<20} {'åŸå§‹ç‚¹æ•°':<12} {'å‹ç¼©åç‚¹æ•°':<12} {'å‹ç¼©æ¯”(%)':<12} "
                   f"{'å‹ç¼©æ—¶é—´(us/ç‚¹)':<18} {'è§£å‹æ—¶é—´(us/ç‚¹)':<18} {'å¹³å‡è¯¯å·®(m)':<15} {'æœ€å¤§è¯¯å·®(m)':<15}\n")
            f.write("-" * 140 + "\n")
            
            # è¾“å‡ºæ¯ä¸ªç®—æ³•çš„ç»“æœ
            for algo in ALGORITHMS:
                algo_name = algo['name']
                key = (algo_name, dataset_name)
                
                if key in all_results:
                    r = all_results[key]
                    f.write(f"{algo_name:<20} "
                           f"{int(r['original_points']):<12} "
                           f"{int(r['compressed_points']):<12} "
                           f"{r['compression_ratio']:<12.2f} "
                           f"{r['avg_compress_time']:<18.4f} "
                           f"{r['avg_decompress_time']:<18.4f} "
                           f"{r['avg_error']:<15.2f} "
                           f"{r['max_error']:<15.2f}\n")
                else:
                    f.write(f"{algo_name:<20} {'N/A':<12} {'N/A':<12} {'N/A':<12} "
                           f"{'N/A':<18} {'N/A':<18} {'N/A':<15} {'N/A':<15}\n")
            
            f.write("\n")
        
        # æ±‡æ€»å¯¹æ¯”ï¼ˆæŒ‰ç®—æ³•ï¼‰
        f.write("\n" + "=" * 100 + "\n")
        f.write("ç®—æ³•æ€§èƒ½æ±‡æ€»ï¼ˆæ‰€æœ‰æ•°æ®é›†å¹³å‡ï¼‰\n")
        f.write("=" * 100 + "\n\n")
        
        f.write(f"{'ç®—æ³•':<20} {'å¹³å‡å‹ç¼©æ¯”(%)':<18} {'å¹³å‡å‹ç¼©æ—¶é—´(us/ç‚¹)':<22} "
               f"{'å¹³å‡è§£å‹æ—¶é—´(us/ç‚¹)':<22} {'å¹³å‡è¯¯å·®(m)':<15}\n")
        f.write("-" * 100 + "\n")
        
        for algo in ALGORITHMS:
            algo_name = algo['name']
            
            # è®¡ç®—è¯¥ç®—æ³•åœ¨æ‰€æœ‰æ•°æ®é›†ä¸Šçš„å¹³å‡å€¼
            algo_results = [all_results[(algo_name, ds['name'])] 
                          for ds in DATASETS 
                          if (algo_name, ds['name']) in all_results]
            
            if algo_results:
                avg_ratio = sum(r['compression_ratio'] for r in algo_results) / len(algo_results)
                avg_comp_time = sum(r['avg_compress_time'] for r in algo_results) / len(algo_results)
                avg_decomp_time = sum(r['avg_decompress_time'] for r in algo_results) / len(algo_results)
                avg_err = sum(r['avg_error'] for r in algo_results) / len(algo_results)
                
                f.write(f"{algo_name:<20} "
                       f"{avg_ratio:<18.2f} "
                       f"{avg_comp_time:<22.4f} "
                       f"{avg_decomp_time:<22.4f} "
                       f"{avg_err:<15.2f}\n")
            else:
                f.write(f"{algo_name:<20} {'N/A':<18} {'N/A':<22} {'N/A':<22} {'N/A':<15}\n")
        
        f.write("\n" + "=" * 100 + "\n")
        f.write("æµ‹è¯•å®Œæˆ\n")
        f.write("=" * 100 + "\n")
    
    print(f"\nâœ“ æ±‡æ€»æŠ¥å‘Šå·²ç”Ÿæˆ: {report_file}")
    
    # åŒæ—¶åœ¨æ§åˆ¶å°è¾“å‡º
    with open(report_file, 'r', encoding='utf-8') as f:
        print("\n" + f.read())

def main():
    """
    ä¸»å‡½æ•°
    """
    print("\n" + "=" * 100)
    print("è½¨è¿¹å‹ç¼©ç®—æ³•æ€§èƒ½æµ‹è¯•")
    print("=" * 100 + "\n")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = PROJECT_ROOT / "test_results"
    output_dir.mkdir(exist_ok=True)
    print(f"è¾“å‡ºç›®å½•: {output_dir}\n")
    
    # å­˜å‚¨æ‰€æœ‰æµ‹è¯•ç»“æœ
    all_results = {}
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    total_tests = len(ALGORITHMS) * len(DATASETS)
    current_test = 0
    
    for dataset in DATASETS:
        print(f"\n{'='*100}")
        print(f"æµ‹è¯•æ•°æ®é›†: {dataset['name']}")
        print(f"{'='*100}\n")
        
        for algorithm in ALGORITHMS:
            current_test += 1
            print(f"[{current_test}/{total_tests}] {algorithm['name']} on {dataset['name']}")
            
            result = run_algorithm(algorithm, dataset, output_dir)
            
            if result:
                all_results[(algorithm['name'], dataset['name'])] = result
            
            print()
    
    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    print("\n" + "=" * 100)
    print("ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š...")
    print("=" * 100)
    
    # ç”ŸæˆTXTæ ¼å¼æŠ¥å‘Š
    generate_report(all_results, output_dir)
    
    # ç”ŸæˆCSVæ ¼å¼æŠ¥å‘Š
    generate_csv_report(all_results, output_dir)
    
    print("\nâœ“ æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nâš ï¸  æµ‹è¯•è¢«ç”¨æˆ·ä¸­æ–­")
        sys.exit(1)
    except Exception as e:
        print(f"\n\nâŒ å‘ç”Ÿé”™è¯¯: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

